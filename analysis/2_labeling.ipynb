{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ce2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "messages = pd.read_csv(\"data/messages.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c22c80",
   "metadata": {},
   "source": [
    "# Data labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a9e41e",
   "metadata": {},
   "source": [
    "## Input preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e81d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = messages[(messages.section=='follow_up')][['id', 'role', 'content', 'chatID']].copy(deep=True)\n",
    "copy.rename(columns={\"content\":\"answer\", \"id\":\"message_id\",'chatID':'conversation_id'}, inplace=True)\n",
    "copy['question'] = ''\n",
    "for i, r in copy.iterrows():\n",
    "    if (r.role=='user'):\n",
    "        copy.loc[i, 'question'] = previous.answer\n",
    "    previous = r\n",
    "copy = copy[copy.role=='user'][['message_id','question', 'answer', 'conversation_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af2e8f",
   "metadata": {},
   "source": [
    "### Random shuffle into 50 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "434a7210",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatIDs = copy['conversation_id'].unique()\n",
    "np.random.shuffle(chatIDs)\n",
    "splitIDs = np.array_split(chatIDs, 50)\n",
    "inputTexts = {i: copy[copy['conversation_id'].isin(group_chat_ids)].reset_index(drop=True).to_csv(index=False) for i, group_chat_ids in enumerate(splitIDs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b80403",
   "metadata": {},
   "outputs": [],
   "source": [
    "### shuffle without chatIDs grouping\n",
    "# shuffledIndices = np.random.permutation(copy.index)\n",
    "# splitIndices = np.array_split(shuffledIndices, 50)\n",
    "# inputTexts = {i: copy.loc[idx].reset_index(drop=True).to_csv(index=False) for i, idx in enumerate(splitIndices)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418f96ac",
   "metadata": {},
   "source": [
    "## Automatic labeling (GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa5c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API is an ENV variable\n",
    "client = OpenAI(api_key=API)\n",
    "outputTexts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d998ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Act in the role of a user researcher analyzing conversations between an AI assistant and human participants. Conversations, identified by conversation ID, comprise chronologically ordered questions asked by an AI assistant and answers by the participant, each identified by a message ID. The conversations are from different participants and cover various topics. Assign these measures to each user answer:\n",
    "  1) specificity: how specific is the information given in the answer, on a scale from 0 to 2 (0 = contains general descriptions, 1 = contains specific concepts, 2 = contains specific concepts with detailed examples)\n",
    "  2) relevance: how relevant is the answer to the question being asked, rate on a scale from 0 to 2 (0 = irrelevant, 1 = partially relevant, 2 = highly relevant)\n",
    "  3) clarity: how clear is the participant’s answer, rate on a scale from 0 to 2 (0 = illegible, 1 = incomplete or partially legible, 2 = clear and well-articulated). Consider semantic rather than syntactic clarity. Typos or joined words (e.g., \"dontknow\") don’t lower the score if the response is otherwise clear.\n",
    "  4) self-disclosure: count of unique personal attributes, topics, concepts, or ideas mentioned by the participant in their answer, such as previous experiences, feelings, hobbies or other personal information (0 or more)\n",
    "  5) sentiment: how positive/negative is a participant's answer (1 = positive, -1 = negative, or 0 = neutral) Does the answer express positive/negative attitude toward its subject, or does it describe it neutrally? Do not make assumptions if the sentiment is not sufficiently explicit.\n",
    "\n",
    "Output a CSV file with six columns: \"message_id\" (copied from input), and the five assigned measures. Exclude the original conversation_id, question and answer columns. Ensure valid CSV formatting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc52236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "for id in np.sort(list(inputTexts.keys())):\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        input=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"input_text\",\n",
    "                \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"input_text\",\n",
    "                \"text\": inputTexts[id]\n",
    "                }\n",
    "            ]\n",
    "            }\n",
    "        ],\n",
    "        text={\n",
    "            \"format\": {\n",
    "            \"type\": \"text\"\n",
    "            }\n",
    "        },\n",
    "        reasoning={},\n",
    "        tools=[],\n",
    "        temperature=0.2,\n",
    "        max_output_tokens=2048,\n",
    "        top_p=1,\n",
    "        store=True,\n",
    "        stream=False\n",
    "    )\n",
    "    outputTexts[id] = response.output_text\n",
    "    print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be486cda",
   "metadata": {},
   "source": [
    "## Ouput preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a18cfb",
   "metadata": {},
   "source": [
    "### Trim unneccessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b042d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in outputTexts:\n",
    "    if outputTexts[i][0:6] == '```csv':\n",
    "        outputTexts[i] = outputTexts[i][7:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e710ad",
   "metadata": {},
   "source": [
    "### Create dataframes from text strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45daf765",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDataframes = {}\n",
    "\n",
    "for id in outputTexts:\n",
    "    content = outputTexts[id]\n",
    "    csv_file = io.StringIO(content)\n",
    "    outputDataframes[int(id)] = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee8071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### backup\n",
    "# temp = {}\n",
    "# for id in outputTexts:\n",
    "#     temp[int(id)] = outputTexts[id]\n",
    "# with open('gpt.json', 'w') as f:\n",
    "#     json.dump(temp, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b661c569",
   "metadata": {},
   "source": [
    "### Merge all results and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b49db7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.DataFrame(columns=outputDataframes[8].columns)\n",
    "for id in outputDataframes:\n",
    "    total = pd.concat([total, outputDataframes[id]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a14c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.read_csv(\"data/messages.csv\", index_col=0)\n",
    "messages = messages.merge(total, left_on='id', right_on='message_id', how='left')\n",
    "messages = messages.drop('message_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.to_csv(\"data/messages-auto-labeled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90384515",
   "metadata": {},
   "source": [
    "## Check differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4bd3772",
   "metadata": {},
   "outputs": [],
   "source": [
    "messagesAL = pd.read_csv(\"data/messages-auto-labeled.csv\", index_col=0)\n",
    "messagesL = pd.read_csv(\"data/messages-labeled.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d69f4d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['specificity', 'relevance', 'clarity', 'self-disclosure', 'sentiment']\n",
    "diff_df = messagesL[(messagesL.role=='user') & (messagesL.section=='follow_up')][cols].reset_index(drop=True) - messagesAL[(messagesAL.role=='user') & (messagesAL.section=='follow_up')][cols].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98d36aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.6467991169977925)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(diff_df != 0).sum().sum() / diff_df.size * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "669d822c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.3289183222958054)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(diff_df != 0).any(axis=1).sum() / diff_df.size * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0fcc33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specificity\n",
      "SignificanceResult(statistic=np.float64(0.9706156607774458), pvalue=np.float64(0.0))\n",
      "relevance\n",
      "SignificanceResult(statistic=np.float64(0.9858964393909435), pvalue=np.float64(0.0))\n",
      "clarity\n",
      "SignificanceResult(statistic=np.float64(0.9533903026296588), pvalue=np.float64(0.0))\n",
      "self-disclosure\n",
      "SignificanceResult(statistic=np.float64(0.9153510840071187), pvalue=np.float64(0.0))\n",
      "sentiment\n",
      "SignificanceResult(statistic=np.float64(0.9784212589690885), pvalue=np.float64(0.0))\n"
     ]
    }
   ],
   "source": [
    "for i in cols:\n",
    "    print(i)\n",
    "    print(spearmanr(messagesAL[(messagesAL.role=='user') & (messagesAL.section=='follow_up')][i], messagesL[(messagesL.role=='user') & (messagesL.section=='follow_up')][i].values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
